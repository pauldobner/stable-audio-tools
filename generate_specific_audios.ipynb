{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/paul/miniconda3/envs/stable-audio-ctrl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                }
            ],
            "source": [
                "import json\n",
                "import logging\n",
                "import os\n",
                "import shutil\n",
                "import time\n",
                "from datetime import timedelta\n",
                "\n",
                "import soundfile as sf\n",
                "import torch\n",
                "import torchaudio\n",
                "from einops import rearrange\n",
                "from tqdm import tqdm\n",
                "\n",
                "from stable_audio_tools.inference.generation import generate_diffusion_cond\n",
                "from stable_audio_tools.models.factory import create_model_from_config\n",
                "from stable_audio_tools.models.utils import copy_state_dict, load_ckpt_state_dict\n",
                "from utils.vimsketch_dataset import VimSketchDataset\n",
                "\n",
                "# Constants\n",
                "MODEL_CONFIG_PATH = \"runs/audiocaps_finetune_ctrl/3qfv6n0i/checkpoints/model_config_small_custom.json\"\n",
                "MODEL_CKPT_PATH = \"runs/audiocaps_finetune_ctrl/3qfv6n0i/checkpoints/epoch=2-step=50000.ckpt\"\n",
                "DATASET_ROOT = \"/home/paul/OneDrive/Master/practical_work/Practical-Work-AI-Master/dataset/Vim_Sketch_Dataset/\"\n",
                "\n",
                "# Inference Parameters\n",
                "TRANSFER_STRENGTH = 0.75  # for small model from 0 to 1\n",
                "GUIDANCE_SCALE = 1.0  # 1.0 for rf_denoiser\n",
                "STEPS = 8  # 8 for rf_denoiser\n",
                "SEED = 42\n",
                "TTA = True  # Set to True for Text-to-Audio"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_model(model_config_path, model_ckpt_path, device=\"cuda\"):\n",
                "    print(f\"Loading model config from {model_config_path}\")\n",
                "    with open(model_config_path) as f:\n",
                "        model_config = json.load(f)\n",
                "\n",
                "    print(\"Creating model from config\")\n",
                "    model = create_model_from_config(model_config)\n",
                "\n",
                "    print(f\"Loading model checkpoint from {model_ckpt_path}\")\n",
                "    copy_state_dict(model, load_ckpt_state_dict(model_ckpt_path))\n",
                "\n",
                "    model.to(device).eval().requires_grad_(False)\n",
                "    print(\"Model loaded successfully\")\n",
                "    return model, model_config\n",
                "\n",
                "def save_wave(waveform, savepath, name=\"outwav\", sample_rate=44100):\n",
                "    if type(name) is not list:\n",
                "        name = [name] * waveform.shape[0]\n",
                "\n",
                "    for i in range(waveform.shape[0]):\n",
                "        path = os.path.join(\n",
                "            savepath,\n",
                "            \"%s.wav\"\n",
                "            % (\n",
                "                os.path.basename(name[i])\n",
                "                if (\".wav\" not in name[i])\n",
                "                else os.path.basename(name[i]).split(\".\")[0]\n",
                "            ),\n",
                "        )\n",
                "        print(\"Save audio to %s\" % path)\n",
                "\n",
                "        # Post-processing\n",
                "        audio = waveform[i]\n",
                "\n",
                "        # Peak normalize, clip, convert to int16\n",
                "        audio = (\n",
                "            audio.to(torch.float32)\n",
                "            .div(torch.max(torch.abs(audio)))\n",
                "            .clamp(-1, 1)\n",
                "            .mul(32767)\n",
                "            .to(torch.int16)\n",
                "            .cpu()\n",
                "        )\n",
                "\n",
                "        torchaudio.save(path, audio, sample_rate)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dataset loaded with 12453 items.\n",
                        "Saving to: /home/paul/OneDrive/Master/practical_work/Practical-Work-AI-Master/dataset/Vim_Sketch_Dataset/test/audios\n",
                        "Loading model on cuda\n",
                        "Loading model config from runs/audiocaps_finetune_ctrl/3qfv6n0i/checkpoints/model_config_small_custom.json\n",
                        "Creating model from config\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/paul/miniconda3/envs/stable-audio-ctrl/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
                        "  WeightNorm.apply(module, name, dim)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading model checkpoint from runs/audiocaps_finetune_ctrl/3qfv6n0i/checkpoints/epoch=2-step=50000.ckpt\n",
                        "Model loaded successfully\n",
                        "Starting generation for 12 specific files...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  0%|          | 0/12453 [00:00<?, ?it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Processing: 06724_112-needle_strings-commercial_synthesizers.wav\n",
                        "42\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/tmp/ipykernel_64421/1702962334.py:75: UserWarning: torchaudio._backend.utils.info has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
                        "  info = torchaudio.info(imitation_path)\n",
                        "/home/paul/miniconda3/envs/stable-audio-ctrl/lib/python3.10/site-packages/torchaudio/_backend/soundfile_backend.py:120: UserWarning: torchaudio._backend.common.AudioMetaData has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
                        "  return AudioMetaData(\n",
                        "/home/paul/miniconda3/envs/stable-audio-ctrl/lib/python3.10/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
                        "  warnings.warn(\n",
                        "/home/paul/Projects/stable-audio-tools/stable_audio_tools/models/conditioners.py:365: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
                        "  with torch.cuda.amp.autocast(dtype=torch.float16) and torch.set_grad_enabled(self.enable_grad):\n",
                        "/home/paul/miniconda3/envs/stable-audio-ctrl/lib/python3.10/site-packages/librosa/core/convert.py:1869: RuntimeWarning: divide by zero encountered in log10\n",
                        "  + 2 * np.log10(f_sq)\n",
                        "100%|██████████| 8/8 [00:00<00:00, 48.63it/s]\n",
                        "/home/paul/miniconda3/envs/stable-audio-ctrl/lib/python3.10/site-packages/torchaudio/_backend/utils.py:337: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.save_with_torchcodec` under the hood. Some parameters like format, encoding, bits_per_sample, buffer_size, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's encoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.encoders.AudioEncoder\n",
                        "  warnings.warn(\n",
                        " 72%|███████▏  | 8947/12453 [00:02<00:00, 4362.53it/s]/tmp/ipykernel_64421/1702962334.py:75: UserWarning: torchaudio._backend.utils.info has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
                        "  info = torchaudio.info(imitation_path)\n",
                        "/home/paul/miniconda3/envs/stable-audio-ctrl/lib/python3.10/site-packages/torchaudio/_backend/soundfile_backend.py:120: UserWarning: torchaudio._backend.common.AudioMetaData has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
                        "  return AudioMetaData(\n",
                        "/home/paul/miniconda3/envs/stable-audio-ctrl/lib/python3.10/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
                        "  warnings.warn(\n",
                        "/home/paul/Projects/stable-audio-tools/stable_audio_tools/models/conditioners.py:365: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
                        "  with torch.cuda.amp.autocast(dtype=torch.float16) and torch.set_grad_enabled(self.enable_grad):\n",
                        "/home/paul/miniconda3/envs/stable-audio-ctrl/lib/python3.10/site-packages/librosa/core/convert.py:1869: RuntimeWarning: divide by zero encountered in log10\n",
                        "  + 2 * np.log10(f_sq)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Save audio to /home/paul/OneDrive/Master/practical_work/Practical-Work-AI-Master/dataset/Vim_Sketch_Dataset/test/audios/06724_112-needle_strings-commercial_synthesizers.wav\n",
                        "Successfully generated 06724_112-needle_strings-commercial_synthesizers.wav\n",
                        "Processing: 11764_132-piano_playing-everyday.wav\n",
                        "42\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 8/8 [00:00<00:00, 76.74it/s]\n",
                        " 77%|███████▋  | 9564/12453 [00:02<00:00, 4063.07it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Save audio to /home/paul/OneDrive/Master/practical_work/Practical-Work-AI-Master/dataset/Vim_Sketch_Dataset/test/audios/11764_132-piano_playing-everyday.wav\n",
                        "Successfully generated 11764_132-piano_playing-everyday.wav\n",
                        "Processing: 11873_143-rooster_calling-everyday.wav\n",
                        "42\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 8/8 [00:00<00:00, 78.06it/s]\n",
                        " 79%|███████▉  | 9895/12453 [00:02<00:00, 3734.80it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Save audio to /home/paul/OneDrive/Master/practical_work/Practical-Work-AI-Master/dataset/Vim_Sketch_Dataset/test/audios/11873_143-rooster_calling-everyday.wav\n",
                        "Successfully generated 11873_143-rooster_calling-everyday.wav\n",
                        "Processing: 11883_144-sandpaper_rubbing-everyday.wav\n",
                        "42\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 8/8 [00:00<00:00, 81.43it/s]\n",
                        " 81%|████████▏ | 10148/12453 [00:02<00:00, 3277.38it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Save audio to /home/paul/OneDrive/Master/practical_work/Practical-Work-AI-Master/dataset/Vim_Sketch_Dataset/test/audios/11883_144-sandpaper_rubbing-everyday.wav\n",
                        "Successfully generated 11883_144-sandpaper_rubbing-everyday.wav\n",
                        "Processing: 07233_162-subsynth_2007-single_synthesizer.wav\n",
                        "42\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 8/8 [00:00<00:00, 78.48it/s]\n",
                        " 84%|████████▎ | 10414/12453 [00:02<00:00, 2803.63it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Save audio to /home/paul/OneDrive/Master/practical_work/Practical-Work-AI-Master/dataset/Vim_Sketch_Dataset/test/audios/07233_162-subsynth_2007-single_synthesizer.wav\n",
                        "Successfully generated 07233_162-subsynth_2007-single_synthesizer.wav\n",
                        "Processing: 07266_166-subsynth_2039-single_synthesizer.wav\n",
                        "42\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 8/8 [00:00<00:00, 78.89it/s]\n",
                        " 85%|████████▍ | 10583/12453 [00:03<00:00, 2338.84it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Save audio to /home/paul/OneDrive/Master/practical_work/Practical-Work-AI-Master/dataset/Vim_Sketch_Dataset/test/audios/07266_166-subsynth_2039-single_synthesizer.wav\n",
                        "Successfully generated 07266_166-subsynth_2039-single_synthesizer.wav\n",
                        "Processing: 07573_196-subsynth_9879-single_synthesizer.wav\n",
                        "42\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 8/8 [00:00<00:00, 78.61it/s]\n",
                        " 89%|████████▉ | 11117/12453 [00:03<00:00, 2355.68it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Save audio to /home/paul/OneDrive/Master/practical_work/Practical-Work-AI-Master/dataset/Vim_Sketch_Dataset/test/audios/07573_196-subsynth_9879-single_synthesizer.wav\n",
                        "Successfully generated 07573_196-subsynth_9879-single_synthesizer.wav\n",
                        "Processing: 07602_199-synth_metallic_stars-commercial_synthesizers.wav\n",
                        "42\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 8/8 [00:00<00:00, 51.43it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Save audio to /home/paul/OneDrive/Master/practical_work/Practical-Work-AI-Master/dataset/Vim_Sketch_Dataset/test/audios/07602_199-synth_metallic_stars-commercial_synthesizers.wav\n",
                        "Successfully generated 07602_199-synth_metallic_stars-commercial_synthesizers.wav\n",
                        "Processing: 12048_200-tambourine-acoustic_instruments.wav\n",
                        "42\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 8/8 [00:00<00:00, 81.05it/s]\n",
                        " 91%|█████████ | 11289/12453 [00:04<00:00, 1322.51it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Save audio to /home/paul/OneDrive/Master/practical_work/Practical-Work-AI-Master/dataset/Vim_Sketch_Dataset/test/audios/12048_200-tambourine-acoustic_instruments.wav\n",
                        "Successfully generated 12048_200-tambourine-acoustic_instruments.wav\n",
                        "Processing: 07794_218-vibraphone_sustained-acoustic_instruments.wav\n",
                        "42\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 8/8 [00:00<00:00, 78.28it/s]\n",
                        " 94%|█████████▍| 11768/12453 [00:04<00:00, 1384.51it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Save audio to /home/paul/OneDrive/Master/practical_work/Practical-Work-AI-Master/dataset/Vim_Sketch_Dataset/test/audios/07794_218-vibraphone_sustained-acoustic_instruments.wav\n",
                        "Successfully generated 07794_218-vibraphone_sustained-acoustic_instruments.wav\n",
                        "Processing: 12292_224-water_bubbling-everyday.wav\n",
                        "42\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 8/8 [00:00<00:00, 77.37it/s]\n",
                        " 96%|█████████▌| 11972/12453 [00:04<00:00, 1145.71it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Save audio to /home/paul/OneDrive/Master/practical_work/Practical-Work-AI-Master/dataset/Vim_Sketch_Dataset/test/audios/12292_224-water_bubbling-everyday.wav\n",
                        "Successfully generated 12292_224-water_bubbling-everyday.wav\n",
                        "Processing: 07948_233-windgong-acoustic_instruments.wav\n",
                        "42\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 8/8 [00:00<00:00, 77.81it/s]\n",
                        "100%|██████████| 12453/12453 [00:04<00:00, 2499.48it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Save audio to /home/paul/OneDrive/Master/practical_work/Practical-Work-AI-Master/dataset/Vim_Sketch_Dataset/test/audios/07948_233-windgong-acoustic_instruments.wav\n",
                        "Successfully generated 07948_233-windgong-acoustic_instruments.wav\n",
                        "\n",
                        "Done! Processed 12/12 target files.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Target files to generate\n",
                "target_files = [\n",
                "    'dataset/Vim_Sketch_Dataset/vocal_imitations/06724_112-needle_strings-commercial_synthesizers.wav',\n",
                "    'dataset/Vim_Sketch_Dataset/vocal_imitations/07233_162-subsynth_2007-single_synthesizer.wav',\n",
                "    'dataset/Vim_Sketch_Dataset/vocal_imitations/07266_166-subsynth_2039-single_synthesizer.wav',\n",
                "    'dataset/Vim_Sketch_Dataset/vocal_imitations/07573_196-subsynth_9879-single_synthesizer.wav',\n",
                "    'dataset/Vim_Sketch_Dataset/vocal_imitations/07602_199-synth_metallic_stars-commercial_synthesizers.wav',\n",
                "    'dataset/Vim_Sketch_Dataset/vocal_imitations/07794_218-vibraphone_sustained-acoustic_instruments.wav',\n",
                "    'dataset/Vim_Sketch_Dataset/vocal_imitations/07948_233-windgong-acoustic_instruments.wav',\n",
                "    'dataset/Vim_Sketch_Dataset/vocal_imitations/11764_132-piano_playing-everyday.wav',\n",
                "    'dataset/Vim_Sketch_Dataset/vocal_imitations/11873_143-rooster_calling-everyday.wav',\n",
                "    'dataset/Vim_Sketch_Dataset/vocal_imitations/11883_144-sandpaper_rubbing-everyday.wav',\n",
                "    'dataset/Vim_Sketch_Dataset/vocal_imitations/12048_200-tambourine-acoustic_instruments.wav',\n",
                "    'dataset/Vim_Sketch_Dataset/vocal_imitations/12292_224-water_bubbling-everyday.wav'\n",
                "]\n",
                "# Extract just the filenames for matching\n",
                "target_filenames = set(os.path.basename(f) for f in target_files)\n",
                "\n",
                "# Setup paths\n",
                "dataset_root = os.environ.get(\"DATASET_ROOT\", DATASET_ROOT)\n",
                "\n",
                "dataset = VimSketchDataset(dataset_root)\n",
                "print(f\"Dataset loaded with {len(dataset)} items.\")\n",
                "\n",
                "# Setup save path\n",
                "if TTA:\n",
                "    save_path = os.path.join(dataset_root, \"test\", \"audios\")\n",
                "else:\n",
                "    # Fallback/Not expected for this task\n",
                "    save_path = os.path.join(\n",
                "        dataset_root, \"style_transfer_sao\", f\"transfer_strength_{TRANSFER_STRENGTH}\"\n",
                "    )\n",
                "\n",
                "os.makedirs(save_path, exist_ok=True)\n",
                "print(f\"Saving to: {save_path}\")\n",
                "\n",
                "# Load Model\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Loading model on {device}\")\n",
                "model, model_config = load_model(MODEL_CONFIG_PATH, MODEL_CKPT_PATH, device=device)\n",
                "sample_rate = model_config[\"sample_rate\"]\n",
                "sample_size = model_config[\"sample_size\"]\n",
                "\n",
                "# Start timing\n",
                "start_time = time.time()\n",
                "\n",
                "print(f\"Starting generation for {len(target_filenames)} specific files...\")\n",
                "\n",
                "processed_count = 0\n",
                "\n",
                "for i in tqdm(range(len(dataset))):\n",
                "    imitation_path = dataset[i][\"imitation_path\"]\n",
                "    reference_path = dataset[i][\"reference_path\"]\n",
                "    text = dataset[i][\"text\"]\n",
                "\n",
                "    # Get output filename\n",
                "    imitation_filename = os.path.basename(imitation_path)\n",
                "    \n",
                "    # FILTER: Check if this file is in our target list\n",
                "    if imitation_filename not in target_filenames:\n",
                "        continue\n",
                "        \n",
                "    output_file = os.path.join(save_path, imitation_filename)\n",
                "\n",
                "    # Skip if already processed\n",
                "    if os.path.exists(output_file):\n",
                "        print(f\"Skipping {imitation_filename}, already processed\")\n",
                "        processed_count += 1\n",
                "        continue\n",
                "\n",
                "    try:\n",
                "        print(f\"Processing: {imitation_filename}\")\n",
                "        \n",
                "        # Get audio duration from imitation path\n",
                "        info = torchaudio.info(imitation_path)\n",
                "        duration = info.num_frames / info.sample_rate\n",
                "\n",
                "        # Calculate sample_size based on duration\n",
                "        target_sample_size = int(duration * sample_rate)\n",
                "\n",
                "        # Ensure target_sample_size is valid for the model\n",
                "        if model.pretransform is not None:\n",
                "            downsampling_ratio = model.pretransform.downsampling_ratio\n",
                "            target_sample_size = (\n",
                "                target_sample_size // downsampling_ratio\n",
                "            ) * downsampling_ratio\n",
                "            \n",
                "        # Load audio for control signals\n",
                "        audio_tensor, sr = torchaudio.load(imitation_path)\n",
                "        if sr != sample_rate:\n",
                "            resampler = torchaudio.transforms.Resample(sr, sample_rate)\n",
                "            audio_tensor = resampler(audio_tensor)\n",
                "        \n",
                "        # FIX: Ensure audio_tensor matches target_sample_size\n",
                "        # The model's encoder expects the input audio to align with the target sample size\n",
                "        if audio_tensor.shape[-1] > target_sample_size:\n",
                "            audio_tensor = audio_tensor[..., :target_sample_size]\n",
                "        elif audio_tensor.shape[-1] < target_sample_size:\n",
                "            # Pad with zeros\n",
                "            pad_size = target_sample_size - audio_tensor.shape[-1]\n",
                "            audio_tensor = torch.nn.functional.pad(audio_tensor, (0, pad_size))\n",
                "\n",
                "        # Prepare conditioning\n",
                "        conditioning = [\n",
                "            {\n",
                "                \"prompt\": text,\n",
                "                \"seconds_start\": 0,\n",
                "                \"seconds_total\": duration,\n",
                "                \"audio\": audio_tensor,\n",
                "            }\n",
                "        ]\n",
                "\n",
                "        # Determine sampler parameters\n",
                "        diffusion_objective = model.diffusion_objective\n",
                "        if diffusion_objective == \"rf_denoiser\":\n",
                "            sampler_type = \"pingpong\"\n",
                "            sigma_min = 0.01\n",
                "            sigma_max = 1.0\n",
                "        elif diffusion_objective == \"rectified_flow\":\n",
                "            sampler_type = \"euler\"\n",
                "            sigma_min = 0.01\n",
                "            sigma_max = 1.0\n",
                "        else:\n",
                "            sampler_type = \"dpmpp-3m-sde\"\n",
                "            sigma_min = 0.03\n",
                "            sigma_max = 500\n",
                "\n",
                "        # TTA Logic\n",
                "        # Calculate max duration from model config\n",
                "        max_duration = sample_size / sample_rate\n",
                "\n",
                "        if duration > max_duration:\n",
                "            print(\n",
                "                f\"Requested duration {duration:.2f}s exceeds model max {max_duration:.2f}s, clipping.\"\n",
                "            )\n",
                "            duration = max_duration\n",
                "            # Start over calculation if clipped\n",
                "            target_sample_size = int(duration * sample_rate)\n",
                "            if model.pretransform is not None:\n",
                "                downsampling_ratio = model.pretransform.downsampling_ratio\n",
                "                target_sample_size = (\n",
                "                    target_sample_size // downsampling_ratio\n",
                "                ) * downsampling_ratio\n",
                "\n",
                "        # Generate TTA\n",
                "        output = generate_diffusion_cond(\n",
                "            model,\n",
                "            steps=STEPS,\n",
                "            cfg_scale=GUIDANCE_SCALE,\n",
                "            conditioning=conditioning,\n",
                "            sample_size=target_sample_size,\n",
                "            seed=SEED,\n",
                "            device=device,\n",
                "            init_audio=None,\n",
                "            init_noise_level=1.0,\n",
                "            sampler_type=sampler_type,\n",
                "            sigma_min=sigma_min,\n",
                "            sigma_max=sigma_max,\n",
                "        )\n",
                "\n",
                "        # Trimming\n",
                "        output = output[..., :target_sample_size]\n",
                "\n",
                "        save_wave(\n",
                "            output,\n",
                "            save_path,\n",
                "            name=imitation_filename,\n",
                "            sample_rate=sample_rate,\n",
                "        )\n",
                "        \n",
                "        processed_count += 1\n",
                "        print(f\"Successfully generated {imitation_filename}\")\n",
                "\n",
                "    except Exception as e:\n",
                "        print(f\"Error processing {imitation_filename}: {str(e)}\")\n",
                "        # raise e # Uncomment for debugging\n",
                "        continue\n",
                "\n",
                "print(f\"\\nDone! Processed {processed_count}/{len(target_filenames)} target files.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "stable-audio-ctrl",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.18"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
